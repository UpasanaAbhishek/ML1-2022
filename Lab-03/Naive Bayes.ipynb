{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9b08b5",
   "metadata": {},
   "source": [
    "# Text Classification using Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c92c58",
   "metadata": {},
   "source": [
    "#### Naive Baye’s classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ef8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from  nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b0df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e031c15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ebbbc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping the redundent looking collumns (for this project)\n",
    "to_drop = [\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"]\n",
    "data = data.drop(data[to_drop], axis=1)\n",
    "# Renaming the columns \n",
    "data.rename(columns = {\"v1\":\"Category\", \"v2\":\"Text\"}, inplace = True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb94e2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "277fcf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1422bc9b",
   "metadata": {},
   "source": [
    "#### Tokenization is breaking complex data into smaller units called tokens. It can be done by splitting paragraphs into sentences and sentences into words. I am splitting the Clean_Text into words at this step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1038938c",
   "metadata": {},
   "source": [
    "#### Stopwords are frequently occurring words(such as few, is, an, etc). These words hold meaning in sentence structure, but do not contribute much to language processing in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "643d0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(a):\n",
    "    remove_punctuation = [char for char in a if char not in string.punctuation]\n",
    "    #print(remove_punctuation)\n",
    "    \n",
    "    remove_punctuation = ''.join(remove_punctuation)\n",
    "    #print(remove_punctuation)\n",
    "    \n",
    "    return [ word for word in remove_punctuation.split() if word.lower() not in stopwords.words('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16660074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [Go, jurong, point, crazy, Available, bugis, n...\n",
      "1                          [Ok, lar, Joking, wif, u, oni]\n",
      "2       [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
      "3           [U, dun, say, early, hor, U, c, already, say]\n",
      "4       [Nah, dont, think, goes, usf, lives, around, t...\n",
      "                              ...                        \n",
      "5567    [2nd, time, tried, 2, contact, u, U, �750, Pou...\n",
      "5568                   [�, b, going, esplanade, fr, home]\n",
      "5569                     [Pity, mood, Soany, suggestions]\n",
      "5570    [guy, bitching, acted, like, id, interested, b...\n",
      "5571                                   [Rofl, true, name]\n",
      "Name: Text, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# To print the second column after removing stopwords and tokenization\n",
    "print(data.iloc[:,1].apply(text_cleaning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f3396e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=<function text_cleaning at 0x00000123F9DC3EE0>)\n"
     ]
    }
   ],
   "source": [
    "# In order to give a value to each word so they can be changed to vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer(analyzer=text_cleaning).fit(data['Text'])\n",
    "\n",
    "print(bow_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "115e4fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1096)\t1\n",
      "  (0, 1461)\t1\n",
      "  (0, 2027)\t1\n",
      "  (0, 4574)\t1\n",
      "  (0, 5135)\t1\n",
      "  (0, 5136)\t1\n",
      "  (0, 5685)\t1\n",
      "  (0, 6131)\t1\n",
      "  (0, 6815)\t1\n",
      "  (0, 6846)\t1\n",
      "  (0, 7456)\t1\n",
      "  (0, 7567)\t1\n",
      "  (0, 8231)\t1\n",
      "  (0, 8809)\t1\n",
      "  (0, 10845)\t1\n",
      "  (0, 11043)\t1\n",
      "  (1, 2407)\t1\n",
      "  (1, 3012)\t1\n",
      "  (1, 7600)\t1\n",
      "  (1, 8482)\t1\n",
      "  (1, 10582)\t1\n",
      "  (1, 10952)\t1\n",
      "  (2, 73)\t1\n",
      "  (2, 422)\t1\n",
      "  (2, 429)\t1\n",
      "  :\t:\n",
      "  (5568, 6604)\t1\n",
      "  (5568, 6791)\t1\n",
      "  (5568, 7065)\t1\n",
      "  (5568, 11235)\t1\n",
      "  (5569, 3169)\t1\n",
      "  (5569, 3655)\t1\n",
      "  (5569, 8147)\t1\n",
      "  (5569, 10087)\t1\n",
      "  (5570, 4430)\t1\n",
      "  (5570, 4973)\t1\n",
      "  (5570, 5169)\t1\n",
      "  (5570, 6196)\t1\n",
      "  (5570, 6612)\t1\n",
      "  (5570, 6710)\t1\n",
      "  (5570, 6892)\t1\n",
      "  (5570, 7190)\t1\n",
      "  (5570, 7297)\t1\n",
      "  (5570, 7698)\t1\n",
      "  (5570, 8314)\t1\n",
      "  (5570, 9804)\t1\n",
      "  (5570, 10669)\t1\n",
      "  (5570, 10886)\t1\n",
      "  (5571, 3370)\t1\n",
      "  (5571, 8243)\t1\n",
      "  (5571, 10532)\t1\n"
     ]
    }
   ],
   "source": [
    "title_bow = bow_transformer.transform(data['Text'])\n",
    "\n",
    "print(title_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9989da",
   "metadata": {},
   "source": [
    "#### TF-IDF in NLP stands for Term Frequency – Inverse document frequency. In NLP cleaned data needs to be converted into a numerical format where each word is represented by a matrix. This is also known as word embedding or Word vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31b4f6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer()\n",
      "  (0, 11043)\t0.2302307806673824\n",
      "  (0, 10845)\t0.19070440364977176\n",
      "  (0, 8809)\t0.24700781968848798\n",
      "  (0, 8231)\t0.17135863098645995\n",
      "  (0, 7567)\t0.263992475219973\n",
      "  (0, 7456)\t0.31248959807463006\n",
      "  (0, 6846)\t0.18344049775392818\n",
      "  (0, 6815)\t0.15156099829325625\n",
      "  (0, 6131)\t0.18912594285931972\n",
      "  (0, 5685)\t0.2498079760885523\n",
      "  (0, 5136)\t0.26866384122613163\n",
      "  (0, 5135)\t0.2983050989432094\n",
      "  (0, 4574)\t0.31248959807463006\n",
      "  (0, 2027)\t0.24200168290449323\n",
      "  (0, 1461)\t0.31248959807463006\n",
      "  (0, 1096)\t0.28824103664730155\n",
      "  (1, 10952)\t0.4005697292204744\n",
      "  (1, 10582)\t0.20689436953946386\n",
      "  (1, 8482)\t0.504282830397047\n",
      "  (1, 7600)\t0.37669696082530857\n",
      "  (1, 3012)\t0.29116619142344646\n",
      "  (1, 2407)\t0.5619244500186726\n",
      "  (2, 11003)\t0.1909725859033143\n",
      "  (2, 10964)\t0.15964606600812142\n",
      "  (2, 10570)\t0.1399031851067661\n",
      "  :\t:\n",
      "  (5568, 6791)\t0.31367469776242124\n",
      "  (5568, 6604)\t0.47781076401785183\n",
      "  (5568, 6267)\t0.5575721048646767\n",
      "  (5568, 4801)\t0.3853122086093004\n",
      "  (5569, 10087)\t0.520467167163554\n",
      "  (5569, 8147)\t0.4328299709057074\n",
      "  (5569, 3655)\t0.520467167163554\n",
      "  (5569, 3169)\t0.520467167163554\n",
      "  (5570, 10886)\t0.20433869276865432\n",
      "  (5570, 10669)\t0.228671085678155\n",
      "  (5570, 9804)\t0.22379509128305083\n",
      "  (5570, 8314)\t0.2265094778565237\n",
      "  (5570, 7698)\t0.17261961368438472\n",
      "  (5570, 7297)\t0.30713765248244235\n",
      "  (5570, 7190)\t0.267858170746944\n",
      "  (5570, 6892)\t0.26415555440138505\n",
      "  (5570, 6710)\t0.2941763581843664\n",
      "  (5570, 6612)\t0.20083119898301952\n",
      "  (5570, 6196)\t0.26076186336592394\n",
      "  (5570, 5169)\t0.30234379880988194\n",
      "  (5570, 4973)\t0.3635608230797119\n",
      "  (5570, 4430)\t0.3470581035941143\n",
      "  (5571, 10532)\t0.5423020375645663\n",
      "  (5571, 8243)\t0.48428640947198104\n",
      "  (5571, 3370)\t0.68656767594612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(title_bow)\n",
    "print(tfidf_transformer)\n",
    "\n",
    "title_tfidf = tfidf_transformer.transform(title_bow)\n",
    "print(title_tfidf) #got tfidf values for whole vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acc2e1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 11301)\n"
     ]
    }
   ],
   "source": [
    "print(title_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbb2eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB().fit(title_tfidf,data['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfeffbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "predicted_val = model.predict(title_tfidf)\n",
    "print(predicted_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf0316a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4825,    0],\n",
       "       [ 114,  633]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the confusion matrix of our prediction\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "confusion_matrix(data['Category'],predicted_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "918301d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9795405599425699"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(data['Category'],predicted_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bf2b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c26195",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398447d1",
   "metadata": {},
   "source": [
    "###### https://www.youtube.com/watch?v=oq68P8Kv7nE\n",
    "###### https://towardsdatascience.com/text-classification-using-naive-bayes-theory-a-working-example-2ef4b7eb7d5a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
